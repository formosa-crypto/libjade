
from Jade require "crypto_onetimeauth/poly1305/amd64/ref/poly1305.jinc"
from Jade require "crypto_verify/16/amd64/common/crypto_verify_16.jinc"

u64 five_u64 = 5;
u64 mask26_u64 = 0x3ffffff;
u64 bit25_u64 = 0x1000000;


inline fn __unpack_avx(
  stack u128[5] r12,
  reg u64[3] rt,
  inline int o)
  ->
  stack u128[5]
{
  inline int mask26;
  reg u64 h l;

  mask26 = 0x3ffffff;

  l = rt[0];
  l &= mask26;
  r12[u64 o + 0] = l;

  l = rt[0];
  l >>= 26;
  l &= mask26;
  r12[u64 o + 2] = l;

  l = rt[0];
  l >>= 52;
  h = rt[1];
  h <<= 12;
  l |= h;
  l &= mask26;
  r12[u64 o + 4] = l;

  l = h;
  l >>= 26;
  l &= mask26;
  r12[u64 o + 6] = l;
  
  l = rt[1];
  l >>= 40;
  h = rt[2];
  h <<= 24;
  l |= h;
  r12[u64 o + 8] = l;

  return r12;
}


inline fn __times_5_avx(
  stack u128[5] r12)
  ->
  stack u128[4]
{
  inline int i;
  stack u128[4] r12x5;
  reg u128 t five;

  five = #VPBROADCAST_2u64(five_u64);
  for i=0 to 4
  { t = #VPMULU(five, r12[1+i]);
    r12x5[i] = t; }
  return r12x5;
}


inline fn __broadcast_r2_avx(
  stack u128[5] r12,
  stack u128[4] r12x5)
  ->
  stack u128[5],
  stack u128[4]
{
  inline int i;
  stack u128[5] r22;
  stack u128[4] r22x5;
  reg   u128[5] t;

  for i=0 to 5
  { t[i] = #VPBROADCAST_2u64(r12[u64 2*i]);
    r22[i] = t[i]; }

  for i=0 to 4
  { t[i] = #VPBROADCAST_2u64(r12x5[u64 2*i]);
    r22x5[i] = t[i]; }

  return r22, r22x5;
}


inline fn __broadcast_r4_avx(
  reg u64[3] r4)
  ->
  stack u128[5],
  stack u128[4]
{
  inline int i;
  stack u128[5] r44;
  stack u128[4] r44x5;
  reg u64[5] t;

  r44 = __unpack_avx(r44, r4, 0);

  for i=0 to 5
  { t[i] = r44[u64 2*i]; r44[u64 1+2*i] = t[i]; }

  r44x5 = __times_5_avx(r44);

  return r44, r44x5;
}


inline fn __poly1305_avx_setup(
  reg u64[3] r)
  ->
  stack u128[5], stack u128[4],
  stack u128[5], stack u128[4],
  stack u128[5], stack u128[4]
{
  inline int i;
  stack u128[5] r44 r22 r12;
  stack u128[4] r44x5 r22x5 r12x5;
  reg u64[3] rt;

  // rt = r; store rt
  for i=0 to 2 { rt[i] = r[i]; } rt[2] = 0;
  r12 = __unpack_avx(r12, rt, 1); // r^1

  rt = __mulmod(rt, r); // r^2
  r12 = __unpack_avx(r12, rt, 0);

  r12x5 = __times_5_avx(r12);
  r22, r22x5 = __broadcast_r2_avx(r12, r12x5);

  rt = __mulmod(rt, r); // r^3
  rt = __mulmod(rt, r); // r^4

  r44, r44x5 = __broadcast_r4_avx(rt);

  return r44, r44x5, r22, r22x5, r12, r12x5;
}


inline fn __load_avx(
  reg u64 in,
  reg u128 mask26,
  stack u128 s_bit25)
  ->
  reg u128[5],
  reg u64
{
  reg u128 t;
  reg u128[5] m;

  t = (u128)[in +  0];
  m[3] = (u128)[in + 16];
  m[0] = #VPUNPCKL_2u64(t, m[3]);
  m[3] = #VPUNPCKH_2u64(t, m[3]);
  m[1] = m[0];
  m[2] = m[0];
  m[4] = m[3];
  m[0] &= mask26;
  m[1] >>2u64= 26;
  m[1] &= mask26;
  m[2] >>2u64= 52;
  t = m[3] <<2u64 12;
  m[2] |= t;
  m[2] &= mask26;
  m[3] >>2u64= 14;
  m[3] &= mask26;
  m[4] >>2u64= 40;
  m[4] |= s_bit25;

  in += 32;

  return m, in;
}


inline fn __pack_avx(reg u128[5] h) -> reg u64[3]
{
  reg bool cf;
  reg u128[3] t;
  reg u128[2] u;
  reg u64[3] d r;
  reg u64 c cx4;

  t[0] = h[1] <<2u64 26;
  t[0] +2u64= h[0];
  t[1] = h[3] <<2u64 26;
  t[1] +2u64= h[2];
  t[2] = #VPSRLDQ_128(h[4], 8);
  t[2] +2u64= h[4];
  u[0] = #VPUNPCKL_2u64(t[0], t[1]);
  u[1] = #VPUNPCKH_2u64(t[0], t[1]);
  t[0] = u[0] +2u64 u[1];
  d[0] = #VPEXTR_64(t[0], 0);
  d[1] = #VPEXTR_64(t[0], 1);
  d[2] = #VPEXTR_64(t[2], 0);
  r[0] = d[1];
  r[0] <<= 52;
  r[1] = d[1];
  r[1] >>= 12;
  r[2] = d[2];
  r[2] >>= 24;
  d[2] <<= 40;
  cf, r[0] += d[0];
  cf, r[1] += d[2] + cf;
   _, r[2] += 0 + cf;

  c = r[2];
  cx4 = r[2];
  r[2] &= 3;
  c >>= 2;
  cx4 &= -4;
  c += cx4;

  cf, r[0] += c;
  cf, r[1] += 0 + cf;
   _, r[2] += 0 + cf;

  return r; 
}


inline fn __carry_reduce_avx(
  reg u128[5] x,
  reg u128 mask26)
  ->
  reg u128[5]
{
  reg u128[2] z;
  reg u128 t;

  z[0] = x[0] >>2u64 26;
  z[1] = x[3] >>2u64 26;

  x[0] &= mask26;
  x[3] &= mask26;

  x[1] +2u64= z[0];
  x[4] +2u64= z[1];

  z[0] = x[1] >>2u64 26;
  z[1] = x[4] >>2u64 26;

  t = z[1] <<2u64 2;
  z[1] +2u64= t;

  x[1] &= mask26;
  x[4] &= mask26;
  x[2] +2u64= z[0];
  x[0] +2u64= z[1];

  z[0] = x[2] >>2u64 26;
  z[1] = x[0] >>2u64 26;
  x[2] &= mask26;
  x[0] &= mask26;
  x[3] +2u64= z[0];
  x[1] +2u64= z[1];

  z[0] = x[3] >>2u64 26;
  x[3] &= mask26;
  x[4] +2u64= z[0];

  return x;
}


inline fn __mulmod_avx(
  reg u128[5] h,
  stack u128[5] s_r,
  stack u128[4] s_rx5
) -> reg u128[5]
{
  reg u128[5] t;
  reg u128[4] u;
  reg u128 r0 r1 r4x5 r2 r3x5 r3 r2x5;

  r0 = s_r[0];
  r1 = s_r[1];
  r4x5 = s_rx5[4-1];

  t[0] = #VPMULU(h[0], r0);
  t[1] = #VPMULU(h[1], r0);
  t[2] = #VPMULU(h[2], r0);
  t[3] = #VPMULU(h[3], r0);
  t[4] = #VPMULU(h[4], r0);

  u[0] = #VPMULU(h[0], r1);
  u[1] = #VPMULU(h[1], r1);
  u[2] = #VPMULU(h[2], r1);
  u[3] = #VPMULU(h[3], r1);

  r2 = s_r[2];

  t[1] +2u64= u[0];
  t[2] +2u64= u[1];
  t[3] +2u64= u[2];
  t[4] +2u64= u[3];

  u[0] = #VPMULU(h[1], r4x5);
  u[1] = #VPMULU(h[2], r4x5);
  u[2] = #VPMULU(h[3], r4x5);
  u[3] = #VPMULU(h[4], r4x5);

  r3x5 = s_rx5[3-1];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  u[0] = #VPMULU(h[0], r2);
  u[1] = #VPMULU(h[1], r2);
  u[2] = #VPMULU(h[2], r2);

  r3 = s_r[3];

  t[2] +2u64= u[0];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  u[0] = #VPMULU(h[2], r3x5);
  u[1] = #VPMULU(h[3], r3x5);
  h[2] = #VPMULU(h[4], r3x5);

  r2x5 = s_rx5[2-1];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  h[2] +2u64= t[2];

  u[0] = #VPMULU(h[0], r3);
  u[1] = #VPMULU(h[1], r3);

  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  u[0] = #VPMULU(h[3], r2x5);
  h[1] = #VPMULU(h[4], r2x5);

  t[0] +2u64= u[0];
  h[1] +2u64= t[1];

  u[0] = #VPMULU(h[4], s_rx5[1-1]);
  u[1] = #VPMULU(h[0], s_r[4]);

  h[0] = t[0] +2u64 u[0];
  h[3] = t[3];
  h[4] = t[4] +2u64 u[1];

  return h;
}


inline fn __mainloop_avx_v1(
  reg u128[5] h,
  reg u64 in,
  stack u128[5] s_r44,
  stack u128[4] s_r44x5,
  stack u128[5] s_r22,
  stack u128[4] s_r22x5,
  stack u128 s_mask26 s_bit25)
  ->
  reg u128[5],
  reg u64
{
  stack u128[5] s_h;
  reg u128[5] m;
  reg u128[5] t;
  reg u128[4] u;
  reg u128 r0 r1 r4x5 r2 r3x5 r3 r2x5;
  reg u128 mask26;
  reg u128 m0 m1 mt;

  r0 = s_r44[0];
  r1 = s_r44[1];
  r4x5 = s_r44x5[4-1];

  t[0] = #VPMULU(h[0], r0);
  u[0] = #VPMULU(h[0], r1);
  t[1] = #VPMULU(h[1], r0);
  u[1] = #VPMULU(h[1], r1);
  t[2] = #VPMULU(h[2], r0);
  u[2] = #VPMULU(h[2], r1);
  t[3] = #VPMULU(h[3], r0);   t[1] +2u64= u[0];
  u[3] = #VPMULU(h[3], r1);   t[2] +2u64= u[1];
  t[4] = #VPMULU(h[4], r0);   t[3] +2u64= u[2];
                              t[4] +2u64= u[3];

  u[0] = #VPMULU(h[1], r4x5); m0 = (u128)[in + 0];
  u[1] = #VPMULU(h[2], r4x5); r2 = s_r44[2];
  u[2] = #VPMULU(h[3], r4x5);
  u[3] = #VPMULU(h[4], r4x5);

  t[0] +2u64= u[0];           m1 = (u128)[in + 16];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  u[0] = #VPMULU(h[0], r2);   m[0] = #VPUNPCKL_2u64(m0, m1);
  u[1] = #VPMULU(h[1], r2);   m[3] = #VPUNPCKH_2u64(m0, m1);
  u[2] = #VPMULU(h[2], r2);


  t[2] +2u64= u[0];           r3x5 = s_r44x5[3-1];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  u[0] = #VPMULU(h[2], r3x5); 
  u[1] = #VPMULU(h[3], r3x5); m[1] = m[0]; // h2 dead
  h[2] = #VPMULU(h[4], r3x5); m[1] >>2u64= 26;
                              m[1] &= s_mask26;
                              r3 = s_r44[3];
  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  h[2] +2u64= t[2];

  u[0] = #VPMULU(h[0], r3);   m[4] = m[3]; s_h[2] = h[2];
  u[1] = #VPMULU(h[1], r3);   m[4] >>2u64= 40;
                              m[4] |= s_bit25;
                              r2x5 = s_r44x5[2-1];

  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  u[0] = #VPMULU(h[3], r2x5); m[2] = m[0]; s_h[3] = t[3];
  h[1] = #VPMULU(h[4], r2x5); m[2] >>2u64= 52;
                                  
  t[0] +2u64= u[0];
  h[1] +2u64= t[1];

  u[0] = #VPMULU(h[4], s_r44x5[1-1]); mt = m[3] <<2u64 12; s_h[1] = h[1];
  u[1] = #VPMULU(h[0], s_r44[4]);     m[2] |= mt; mask26 = s_mask26;

  h[0] = t[0] +2u64 u[0];
  h[4] = t[4] +2u64 u[1];

  s_h[0] = h[0];
  s_h[4] = h[4];

  m[0] &= mask26;
  m[2] &= mask26;
  m[3] >>2u64= 14;
  m[3] &= mask26;

  r0 = s_r22[0];
  r1 = s_r22[1];
  r4x5 = s_r22x5[4-1];

  t[0] = #VPMULU(m[0], r0);
  u[0] = #VPMULU(m[0], r1);
  t[1] = #VPMULU(m[1], r0);
  u[1] = #VPMULU(m[1], r1);
  t[2] = #VPMULU(m[2], r0);
  u[2] = #VPMULU(m[2], r1);   t[0] +2u64= s_h[0];
  t[3] = #VPMULU(m[3], r0);   t[1] +2u64= s_h[1]; t[1] +2u64= u[0];
  u[3] = #VPMULU(m[3], r1);   t[2] +2u64= s_h[2]; t[2] +2u64= u[1];
  t[4] = #VPMULU(m[4], r0);   t[3] +2u64= s_h[3]; t[3] +2u64= u[2];
                              t[4] +2u64= s_h[4]; t[4] +2u64= u[3];

  u[0] = #VPMULU(m[1], r4x5); m0 = (u128)[in + 32];
  u[1] = #VPMULU(m[2], r4x5); r2 = s_r22[2];
  u[2] = #VPMULU(m[3], r4x5);
  u[3] = #VPMULU(m[4], r4x5);

  t[0] +2u64= u[0];           m1 = (u128)[in + 48];
  t[1] +2u64= u[1];
  t[2] +2u64= u[2];
  t[3] +2u64= u[3];

  u[0] = #VPMULU(m[0], r2);   h[0] = #VPUNPCKL_2u64(m0, m1);
  u[1] = #VPMULU(m[1], r2);   h[3] = #VPUNPCKH_2u64(m0, m1);
  u[2] = #VPMULU(m[2], r2);


  t[2] +2u64= u[0];           r3x5 = s_r22x5[3-1];
  t[3] +2u64= u[1];
  t[4] +2u64= u[2];

  u[0] = #VPMULU(m[2], r3x5);
  u[1] = #VPMULU(m[3], r3x5); h[1] = h[0];
  m[2] = #VPMULU(m[4], r3x5); h[1] >>2u64= 26;
                              h[1] &= s_mask26;
                              r3 = s_r22[3];

  t[0] +2u64= u[0];
  t[1] +2u64= u[1];
  m[2] +2u64= t[2];

  u[0] = #VPMULU(m[0], r3);   h[4] = h[3];
  u[1] = #VPMULU(m[1], r3);   h[4] >>2u64= 40;
                              h[4] |= s_bit25;
                              r2x5 = s_r22x5[2-1];
  t[3] +2u64= u[0];
  t[4] +2u64= u[1];

  u[0] = #VPMULU(m[3], r2x5); h[2] = h[0];
  m[1] = #VPMULU(m[4], r2x5); h[2] >>2u64= 52;

  t[0] +2u64= u[0];
  m[1] +2u64= t[1];

  u[0] = #VPMULU(m[4], s_r22x5[1-1]); mt = h[3] <<2u64 12;
  u[1] = #VPMULU(m[0], s_r22[4]);     h[2] |= mt; mask26 = s_mask26;

  m[0] = t[0] +2u64 u[0];
  m[3] = t[3];
  m[4] = t[4] +2u64 u[1];

  h[0] &= mask26;  h[0] +2u64= m[0];
  h[2] &= mask26;  h[2] +2u64= m[2];
  h[3] >>2u64= 14; 
  h[3] &= mask26;  h[3] +2u64= m[3];
  in += 64;

  h[1] +2u64= m[1];
  h[4] +2u64= m[4];
  
  h = __carry_reduce_avx(h, mask26);

  return h,  in;
}


inline fn __final_avx_v0(
  reg u128[5] h,
  stack u128[5] s_r,
  stack u128[4] s_rx5,
  stack u128 s_mask26)
  ->
  reg u128[5]
{
  reg u128 mask26;

  h = __mulmod_avx(h, s_r, s_rx5);
  mask26 = s_mask26;
  h = __carry_reduce_avx(h, mask26);

  return h;
}


inline fn __poly1305_avx_update(
  reg u64 in inlen,
  stack u128[5] r44,  
  stack u128[4] r44x5,
  stack u128[5] r22,  
  stack u128[4] r22x5,
  stack u128[5] r12,
  stack u128[4] r12x5)
  ->
  reg u64,
  reg u64,
  reg u64[3]
{
  inline int i;
  stack u128 s_mask26 s_bit25;
  reg u128[5] h;
  reg u128 mask26 t;
  reg u64[3] h64;

  for i=0 to 5
  { h[i] = #set0_128(); }
  t = #VPBROADCAST_2u64(mask26_u64); s_mask26 = t; mask26 = t;
  t = #VPBROADCAST_2u64(bit25_u64); s_bit25 = t;

  while(inlen >= 64)
  { 
    h, in = __mainloop_avx_v1(h, in, r44, r44x5, r22, r22x5, s_mask26, s_bit25);
    inlen -= 64;
  }

  h = __final_avx_v0(h, r12, r12x5, s_mask26);
  h64 = __pack_avx(h);

  return in, inlen, h64;
}


inline fn __poly1305_r_avx(reg u64 in inlen k) -> reg u64[2]
{
  reg u64[2] h2;
  reg u64[3] h r;
  stack u128[5] r44   r22   r12;
  stack u128[4] r44x5 r22x5 r12x5;

  if(inlen >= 1024)
  { h, r, k = __poly1305_setup_ref(k);
    r44, r44x5, r22, r22x5, r12, r12x5 = __poly1305_avx_setup(r);
    in, inlen, h = __poly1305_avx_update(in, inlen, r44, r44x5, r22, r22x5, r12, r12x5);
    in, inlen, h = __poly1305_update_ref(in, inlen, h, r);
    h2 = __poly1305_last_ref(in, inlen, k, h, r);
  }
  else
  { h2 = __poly1305_r_ref(in, inlen, k); }

  return h2;
}


inline fn __poly1305_avx(reg u64 out in inlen k)
{
  reg u64[2] h2;
  h2 =  __poly1305_r_avx(in, inlen, k);
  __store2(out, h2);
}


inline fn __poly1305_verify_avx(reg u64 h in inlen k) -> reg u64
{
  reg u64[2] hc;
  reg u64 r;

  hc = __poly1305_r_avx(in, inlen, k);
  r = __crypto_verify_p_u8x16_r_u64x2(h, hc);
  return r;
}

