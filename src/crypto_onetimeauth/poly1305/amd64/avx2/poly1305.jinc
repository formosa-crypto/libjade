
from Jade require "crypto_onetimeauth/poly1305/amd64/ref/poly1305.jinc"
from Jade require "crypto_verify/16/amd64/common/crypto_verify_16.jinc"

u64 five_u64 = 5;
u64 mask26_u64 = 0x3ffffff;
u64 bit25_u64 = 0x1000000;


inline fn __unpack_avx2(
  stack u256[5] r1234,
  reg u64[3] rt,
  inline int o)
  ->
  stack u256[5]
{
  inline int mask26;
  reg u64 h l;

  mask26 = 0x3ffffff;

  l = rt[0];
  l &= mask26;
  r1234[u64 o + 0] = l;

  l = rt[0];
  l >>= 26;
  l &= mask26;
  r1234[u64 o + 4] = l;

  l = rt[0];
  l >>= 52;
  h = rt[1];
  h <<= 12;
  l |= h;
  l &= mask26;
  r1234[u64 o + 8] = l;

  l = h;
  l >>= 26;
  l &= mask26;
  r1234[u64 o + 12] = l;

  l = rt[1];
  l >>= 40;
  h = rt[2];
  h <<= 24;
  l |= h;
  r1234[u64 o + 16] = l;

  return r1234;
}


inline fn __times_5_avx2(
  stack u256[5] r1234)
  ->
  stack u256[4]
{
  inline int i;
  stack u256[4] r1234x5;
  reg u256 t five;

  five = #VPBROADCAST_4u64(five_u64);
  for i=0 to 4
  { t = #VPMULU_256(five, r1234[1+i]);
    r1234x5[i] = t; }

  return r1234x5;
}


inline fn __broadcast_r4_avx2(
  stack u256[5] r1234,
  stack u256[4] r1234x5)
  ->
  stack u256[5],
  stack u256[4]
{
  inline int i;
  stack u256[5] r4444;
  stack u256[4] r4444x5;
  reg   u256[5] t;

  for i=0 to 5
  { t[i] = #VPBROADCAST_4u64(r1234[u64 4*i]);
    r4444[i] = t[i]; }

  for i=0 to 4
  { t[i] = #VPBROADCAST_4u64(r1234x5[u64 4*i]);
    r4444x5[i] = t[i]; }

  return r4444, r4444x5;
}


inline fn __poly1305_avx2_setup(
  reg u64[3] r)
  ->
  stack u256[5],
  stack u256[4],
  stack u256[5],
  stack u256[4]
{
  inline int i;
  stack u256[5] r4444   r1234;
  stack u256[4] r4444x5 r1234x5;
  reg u64[3] rt;

  // rt = r; store rt
  for i=0 to 2 { rt[i] = r[i]; } rt[2] = 0;
  // r^1
  r1234 = __unpack_avx2(r1234, rt, 3);

  // precompute r^2 r^3 r^4
  for i=0 to 3
  { rt = __mulmod(rt, r);
    r1234 = __unpack_avx2(r1234, rt, 2-i);
  }

  // compute r1234x5
  r1234x5 = __times_5_avx2(r1234);

  // broadcast r^4 and r^4*5 from r1234 r1234x5 into r4444 and r4444x5
  r4444, r4444x5 = __broadcast_r4_avx2(r1234, r1234x5);

  return r4444, r4444x5, r1234, r1234x5;
}


inline fn __load_avx2(
  reg u64 in,
  reg u256 mask26,
  stack u256 s_bit25)
  ->
  reg u256[5],
  reg u64
{
  reg u256[5] m;
  reg u256 t;

  t    = (u256)[in + 0];
  m[1] = (u256)[in + 32];
  in += 64;
  //    t = { in[128:257] , in[0:127]   }
  // m[1] = { in[384:511] , in[256:383] }

  m[0] = #VPERM2I128(t, m[1], (2u4)[2,0]); //0x20
  m[1] = #VPERM2I128(t, m[1], (2u4)[3,1]); //0x31
  // m[0] = { in[256:383] , in[0:127] }
  // m[1] = { in[384:511] , in[128:257] }

  m[2] = #VPSRLDQ_256(m[0], 6);
  m[3] = #VPSRLDQ_256(m[1], 6);
  // m[2] = { in[304:383], in[48:127] }
  // m[3] = { in[432:511], in[176:257] }

  m[4] = #VPUNPCKH_4u64(m[0], m[1]);
  m[0] = #VPUNPCKL_4u64(m[0], m[1]);
  // m[4] = { in[384+64:511], in[256+64:383], in[128+64:257], in[0+64:127] }
  //        { in[448:511]   , in[320:383]   , in[192:257]   , in[64:127  ] }
  //
  // m[0] = { in[384:511-64], in[256:383-64], in[128:257-64], in[0:127-64] }
  //        { in[384:447]   , in[256:319]   , in[128:191]   , in[0:63]     }

  m[3] = #VPUNPCKL_4u64(m[2], m[3]);
  // m[3] = { in[432:495], in[304:367], in[176:239], in[48:111] }

  m[2] = m[3] >>4u64 4;
  // m[2] = { in[436:495], in[308:367], in[180:239], in[52:111] }

  m[2] &= mask26;
  // m[2] = { in[436:461], in[308:333], in[180:205], in[52:77] }

  m[1] = m[0] >>4u64 26;
  // m[1] = { in[410:447], in[282:319], in[154:191], in[26:63] }

  m[0] &= mask26;
  // m[0] = { in[384:409], in[256:281], in[128:153], in[0:25]  }

  m[3] >>4u64= 30;
  // m[3] = { in[462:495], in[334:367], in[206:239], in[78:111] }

  m[3] &= mask26;
  // m[3] = { in[462:487], in[334:359], in[206:231], in[78:103] }

  m[4] >>4u64= 40;
  // m[4] = { in[488:511], in[380:383], in[232:257], in[104:127] }

  m[4] |= s_bit25;

  m[1] &= mask26;
  // m[1] = { in[410:435], in[282:307], in[154:179], in[26:51] }

  return m, in;
}


inline fn __pack_avx2(reg u256[5] h) -> reg u64[3]
{
  reg bool cf;
  reg u256[3] t;
  reg u128 t0;
  reg u256[2] u;
  reg u64[3] d r;
  reg u64 c cx4;

  // t0 = { a3+b3*2^26, a2+b2*2^26, a1+b1*2^26, a0+b0*2^26 }
  //      { ab(3), ab(2), ab(1), ab(0) }
  t[0] = h[1] <<4u64 26;
  t[0] +4u64= h[0];

  // t1 = { c3+d3*2^26, c2+d2*2^26, c1+d1*2^26, c0+d0*2^26 }
  //      { cd(3), cd(2), cd(1), cd(0) }
  t[1] = h[3] <<4u64 26;
  t[1] +4u64= h[2];

  // t2 = { e3, e(2+3), e1, e(0+1) }
  t[2] = #VPSRLDQ_256(h[4], 8);
  t[2] +4u64= h[4];

  // t2 = { e(2+3), e(0+1), xxx, yyy }
  t[2] = #VPERMQ(t[2], (4u2)[2,0,0,0]);

  // u0 = { cd(1), cd(0), ab(1), ab(0) }
  // u1 = { cd(3), cd(2), ab(3), ab(2) }
  u[0] = #VPERM2I128(t[0], t[1], (2u4)[2,0]);
  u[1] = #VPERM2I128(t[0], t[1], (2u4)[3,1]);

  // t0 = { cd(1+3), cd(0+2), ab(1+3), ab(0+2) }
  t[0] = u[0] +4u64 u[1];

  // u0 = { e(0+1), cd(0+2), yyy, ab(0+2) }
  // u1 = { e(2+3), cd(1+3), xxx, ab(1+3) }
  u[0] = #VPUNPCKL_4u64(t[0], t[2]);
  u[1] = #VPUNPCKH_4u64(t[0], t[2]);

  // t0 = { e(0+1+2+3), cd(0+2+1+3), yyy+yyy, ab(0+2+1+3) }
  t[0] = u[0] +4u64 u[1];
  
  // extract t0 values into reg u64
  t0 = #VEXTRACTI128(t[0], 1);
  d[0] = #VPEXTR_64(t[0], 0); // ~55 bits
  d[1] = #VPEXTR_64(t0,   0); // ~55 bits
  d[2] = #VPEXTR_64(t0,   1); // ~29 bits

  // at this point we have that 
  //   R = r0*2^0 + r1^2^52 + r2*2^104
  // and we want it to be
  //   R = r0*2^0 + r1*2^64 + r2*2^128

  r[0] = d[1];
  r[0] <<= 52; // 12 bits from d[1]

  r[1] = d[1];
  r[1] >>= 12; // 52 bits from d[1] (only ~43 should be set)

  r[2] = d[2];
  r[2] >>= 24; // 128 - 104

  d[2] <<= 40; // 64 - (128 - 104)

  cf, r[0] += d[0];
  cf, r[1] += d[2] + cf;
   _, r[2] += 0 + cf;

  // reduce (check mulmod)
  c = r[2];
  cx4 = r[2];
  r[2] &= 3; // clear the remaining bits
  c >>= 2;   // (r[2]>>2)
  cx4 &= -4; // clear first 2 bits: (r[2]>>2)<<2
  c += cx4; 

  cf, r[0] += c;
  cf, r[1] += 0 + cf;
   _, r[2] += 0 + cf;

  return r; 
}


inline fn __carry_reduce_avx2(
  reg u256[5] x,
  reg u256 mask26)
  ->
  reg u256[5]
{
  reg u256[2] z;
  reg u256 t;

  z[0] = x[0] >>4u64 26;
  z[1] = x[3] >>4u64 26;

  x[0] &= mask26;
  x[3] &= mask26;

  x[1] +4u64= z[0];
  x[4] +4u64= z[1];

  z[0] = x[1] >>4u64 26;
  z[1] = x[4] >>4u64 26;

  t = z[1] <<4u64 2;
  z[1] +4u64= t;

  x[1] &= mask26;
  x[4] &= mask26;
  x[2] +4u64= z[0];
  x[0] +4u64= z[1];

  z[0] = x[2] >>4u64 26;
  z[1] = x[0] >>4u64 26;
  x[2] &= mask26;
  x[0] &= mask26;
  x[3] +4u64= z[0];
  x[1] +4u64= z[1];

  z[0] = x[3] >>4u64 26;
  x[3] &= mask26;
  x[4] +4u64= z[0];

  return x;
}


// To compute:
// - H = (H+M)*R
//
// the first step is to:
// - H = H+M
//
// for the multiplication:
// - there are 5 limbs (26 bits each)
//
// with:
// - h = h0*2^0 + h1*2^26 + h2*2^52 + h3*2^78 + h4*2^104
// - r = r0*2^0 + r1*2^26 + r2*2^52 + r3*2^78 + r4*2^104
//
// h * r can be expanded into:
//
//  (h0*r0) * 2^0 + 
//  (h0*r1 + h1*r0) * 2^26 +
//  (h0*r2 + h1*r1 + h2*r0) * 2^52 +
//  (h0*r3 + h1*r2 + h2*r1 + h3*r0) * 2^78 +
//  (h0*r4 + h1*r3 + h2*r2 + h3*r1 + h4*r0) * 2^104 +
//  (h1*r4 + h2*r3 + h3*r2 + h4*r1) * 2^130 +
//  (h2*r4 + h3*r3 + h4*r2) * 2^156 +
//  (h3*r4 + h4*r3) * 2^182 +
//  (h4*r4) * 2^208
//
// if we multiply by 5 "anything above" 2^130 (p = 2^130 - 5)
//
//  (h0*r0 + h1*r4*5 + h2*r3*5 + h3*r2*5 + h4*r1*5) * 2^0 + 
//  (h0*r1 + h1*r0   + h2*r4*5 + h3*r3*5 + h4*r2*5) * 2^26 +
//  (h0*r2 + h1*r1   + h2*r0   + h3*r4*5 + h4*r3*5) * 2^52 +
//  (h0*r3 + h1*r2   + h2*r1   + h3*r0   + h4r4*5 ) * 2^78 +
//  (h0*r4 + h1*r3   + h2*r2   + h3*r1   + h4*r0  ) * 2^104
//
// r and r*5 usage is the following:
//
//  5 times : r0 
//  4 times : r1 and r4*5
//  3 times : r2 and r3*5
//  2 times : r3 and r2*5
//  1 time  : r4 and r1*5
//
// the previous expression can be rearranged acordingly:
//
//  (h0*r0 +         h1*r4*5 +           h2*r3*5 +         h3*r2*5 + h4*r1*5) * 2^0 + 
//  (h1*r0 + h0*r1 + h2*r4*5 +           h3*r3*5 +         h4*r2*5          ) * 2^26 +
//  (h2*r0 + h1*r1 + h3*r4*5 + h0*r2   + h4*r3*5                            ) * 2^52 +
//  (h3*r0 + h2*r1 + h4*r4*5 + h1*r2   +         + h0*r3                    ) * 2^78 +
//  (h4*r0 + h3*r1 +           h2*r2   +         + h1*r3 +           h0*r4  ) * 2^104
inline fn __add_mulmod_avx2(
  reg u256[5] h m,
  stack u256[5] s_r,
  stack u256[4] s_rx5) -> reg u256[5]
{
  reg u256[5] t;
  reg u256[4] u;
  reg u256 r0 r1 r4x5 r2 r3x5 r3 r2x5;
  inline int i;

 r0 = s_r[0];
 r1 = s_r[1];
 r4x5 = s_rx5[4-1];

  // H += M
  for i=0 to 5
  { h[i] +4u64= m[i]; }

  // T = H * r0 (1st col.)
  for i=0 to 5
  { t[i] = #VPMULU_256(h[i], r0); }

  // U = H[0..3] * r1 (2nd col.)
  for i=0 to 4
  { u[i] = #VPMULU_256(h[i], r1); }

 r2 = s_r[2];

  // T[1..4] += U (add first and second col.)
  for i=0 to 4
  { t[i+1] +4u64= u[i]; }

  // U = H[1..4] * r4*5 (3rd col.)
  for i=0 to 4
  { u[i] = #VPMULU_256(h[i+1], r4x5); }

 r3x5 = s_rx5[3-1];

  // T[0..3] += U
  for i=0 to 4
  { t[i] +4u64= u[i]; }

  // U[0..2] = H[0..2] * r2 (4th col.)
  for i=0 to 3
  { u[i] = #VPMULU_256(h[i], r2); }

 r3 = s_r[3];

  // T[2..4] += U
  for i=0 to 3
  { t[2+i] +4u64= u[i]; }


 // H[2..4] * r3*5 (5th col.)
  u[0] = #VPMULU_256(h[2], r3x5); // h2 dead
  u[1] = #VPMULU_256(h[3], r3x5);
  h[2] = #VPMULU_256(h[4], r3x5);

 r2x5 = s_rx5[2-1];

  t[0] +4u64= u[0];
  t[1] +4u64= u[1];
  h[2] +4u64= t[2]; // t2 dead // h2 done

  // H[0.1] * r3 (6th col.)
  u[0] = #VPMULU_256(h[0], r3);
  u[1] = #VPMULU_256(h[1], r3); // h1 dead

  t[3] +4u64= u[0];
  t[4] +4u64= u[1];

  // H[3..4] * r2*5 (7th col.)
  u[0] = #VPMULU_256(h[3], r2x5); // h3 dead
  h[1] = #VPMULU_256(h[4], r2x5);

  t[0] +4u64= u[0];
  h[1] +4u64= t[1]; // t1 dead // h1 done

  // H[4] * r1*5 and H[0] * r4 (8th col.)
  u[0] = #VPMULU_256(h[4], s_rx5[1-1]); // h4 dead
  u[1] = #VPMULU_256(h[0], s_r[4]); // h0 dead

  h[0] = t[0] +4u64 u[0];
  h[3] = t[3];
  h[4] = t[4] +4u64 u[1];

  return h;
}


inline fn __mainloop_avx2_v0(
  reg u256[5] h m,
  reg u64 in,
  stack u256[5] s_r,
  stack u256[4] s_rx5,
  stack u256 s_mask26
             s_bit25
) -> reg u256[5],
     reg u256[5],
     reg u64
{
  reg u256 mask26;

  h = __add_mulmod_avx2(h, m, s_r, s_rx5);
  mask26 = s_mask26;
  h = __carry_reduce_avx2(h, mask26);
  m, in = __load_avx2(in, mask26, s_bit25);

  return h, m, in;
}


// improved version of __mainloop_avx2_v0
inline fn __mainloop_avx2_v1(
  reg u256[5] h m,
  reg u64 in,
  stack u256[5] s_r,
  stack u256[4] s_rx5,
  stack u256 s_mask26
             s_bit25)
  ->
  reg u256[5],
  reg u256[5],
  reg u64
{
  reg u256[5] t;
  reg u256[4] u;
  reg u256[2] z;
  reg u256 z0 m0 r0 r1 r4x5 r2 r3x5 r3 r2x5;
  reg u256 mask26;

                                  r0 = s_r[0];
                                  r1 = s_r[1];
                                  r4x5 = s_rx5[4-1];

  // H += M
  // T = H * r0
  // T[1..4] += H[0..3] * r1
                                  h[0] +4u64= m[0];
                                  h[1] +4u64= m[1];
  t[0] = #VPMULU_256(h[0], r0);   h[2] +4u64= m[2]; 
  u[0] = #VPMULU_256(h[0], r1);   h[3] +4u64= m[3];
  t[1] = #VPMULU_256(h[1], r0);   h[4] +4u64= m[4];
  u[1] = #VPMULU_256(h[1], r1);
  t[2] = #VPMULU_256(h[2], r0);
  u[2] = #VPMULU_256(h[2], r1);
  t[3] = #VPMULU_256(h[3], r0);   t[1] +4u64= u[0];
  u[3] = #VPMULU_256(h[3], r1);   t[2] +4u64= u[1];
  t[4] = #VPMULU_256(h[4], r0);   t[3] +4u64= u[2];
                                  t[4] +4u64= u[3];

  // T[0..3] += H[1..4] * r4*5                                      
  u[0] = #VPMULU_256(h[1], r4x5); m0 = (u256)[in + 0];
  u[1] = #VPMULU_256(h[2], r4x5); r2 = s_r[2];
  u[2] = #VPMULU_256(h[3], r4x5);
  u[3] = #VPMULU_256(h[4], r4x5);
  t[0] +4u64= u[0];               m[1] = (u256)[in + 32];
  t[1] +4u64= u[1];
  t[2] +4u64= u[2];
  t[3] +4u64= u[3];

  // T[2..4] = H[0..2] * r2
  u[0] = #VPMULU_256(h[0], r2);   m[0] = #VPERM2I128(m0, m[1], (2u4)[2,0]);
  u[1] = #VPMULU_256(h[1], r2);   m[1] = #VPERM2I128(m0, m[1], (2u4)[3,1]);
  u[2] = #VPMULU_256(h[2], r2);
  t[2] +4u64= u[0];               r3x5 = s_rx5[3-1];
  t[3] +4u64= u[1];
  t[4] +4u64= u[2];

  // T[0..2] = H[2..4] * r3*5
  u[0] = #VPMULU_256(h[2], r3x5); // h2 dead
  u[1] = #VPMULU_256(h[3], r3x5); r3 = s_r[3];
  h[2] = #VPMULU_256(h[4], r3x5); m[2] = #VPSRLDQ_256(m[0], 6);
  t[0] +4u64= u[0];               m[3] = #VPSRLDQ_256(m[1], 6);
  t[1] +4u64= u[1];
  h[2] +4u64= t[2]; // t2 dead // h2 done

  // T[3..4] += H[0..1] * r3
                                  r2x5 = s_rx5[2-1];
  u[0] = #VPMULU_256(h[0], r3);
  u[1] = #VPMULU_256(h[1], r3);   m[4] = #VPUNPCKH_4u64(m[0], m[1]);
                                  m[0] = #VPUNPCKL_4u64(m[0], m[1]);
  t[3] +4u64= u[0];
  t[4] +4u64= u[1];

  // T[0..1] += H[3..4] * r2*5
  u[0] = #VPMULU_256(h[3], r2x5); // h3 dead
  h[1] = #VPMULU_256(h[4], r2x5);
  t[0] +4u64= u[0];
  h[1] +4u64= t[1]; // t1 dead // h1 done

  mask26 = s_mask26;

  
  // H[4] * r1*5 and H[0] * r4 (8th col.)
  u[0] = #VPMULU_256(h[4], s_rx5[1-1]); // h4 dead
  u[1] = #VPMULU_256(h[0], s_r[4]); // h0 dead
                                  m[3] = #VPUNPCKL_4u64(m[2], m[3]);
                                  m[2] = m[3] >>4u64 4;
  h[0] = t[0] +4u64 u[0];


  z[0] = h[0] >>4u64 26;
  h[0] &= mask26;
  //h[3] = t[3];
  h[3] = t[3] & mask26;
  z[1] = t[3] >>4u64 26;
  h[4] = t[4] +4u64 u[1];

  m[2] &= mask26;
  m[1] = m[0] >>4u64 26;


  h[1] +4u64= z[0];
  h[4] +4u64= z[1];

  z[0] = h[1] >>4u64 26;
  z[1] = h[4] >>4u64 26;

  z0 = z[1] <<4u64 2;
  z[1] +4u64= z0;

  h[1] &= mask26;
  h[4] &= mask26;
  h[2] +4u64= z[0];
  h[0] +4u64= z[1];

  z[0] = h[2] >>4u64 26;
  z[1] = h[0] >>4u64 26;
  h[2] &= mask26;
  h[0] &= mask26;
  h[3] +4u64= z[0];
  h[1] +4u64= z[1];

  z[0] = h[3] >>4u64 26;
  h[3] &= mask26;
  h[4] +4u64= z[0];

  in += 64;
  m[0] &= mask26;
  m[3] >>4u64= 30;
  m[3] &= mask26;
  m[4] >>4u64= 40;
  m[4] |= s_bit25;
  m[1] &= mask26;

  return h, m, in;
}


inline fn __final_avx2_v0(
  reg u256[5] h m,
  stack u256[5] s_r,
  stack u256[4] s_rx5,
  stack u256 s_mask26
) -> reg u256[5]
{
  reg u256 mask26;

  h = __add_mulmod_avx2(h, m, s_r, s_rx5);
  mask26 = s_mask26;
  h = __carry_reduce_avx2(h, mask26);

  return h;
}


inline fn __poly1305_avx2_update(
  reg u64 in inlen,
  stack u256[5] r4444,  
  stack u256[4] r4444x5,
  stack u256[5] r1234,
  stack u256[4] r1234x5)
  ->
  reg u64,
  reg u64,
  reg u64[3]
{
  inline int i;
  stack u256 s_mask26 s_bit25;
  reg u256[5] h m;
  reg u256 mask26 t;
  reg u64[3] h64;

  for i=0 to 5
  { h[i] = #set0_256(); }

  t = #VPBROADCAST_4u64(mask26_u64); s_mask26 = t;
                                       mask26 = t;
  t = #VPBROADCAST_4u64(bit25_u64);  s_bit25  = t;

  // load 64 bytes from input
  m, in = __load_avx2(in, mask26, s_bit25);

  while(inlen >= 128)
  { h, m, in = __mainloop_avx2_v1(h, m, in, r4444, r4444x5, s_mask26, s_bit25);
    inlen -= 64;
  }
  inlen -= 64;

  h = __final_avx2_v0(h, m, r1234, r1234x5, s_mask26);
  h64 = __pack_avx2(h);

  return in, inlen, h64;
}


// TODO: remove inline when ra
inline fn __poly1305_r_avx2(reg u64 in inlen k) -> reg u64[2]
{
  reg u64[2] h2;
  reg u64[3] h r;
  stack u256[5] r4444   r1234;
  stack u256[4] r4444x5 r1234x5;

  if(inlen >= 256)
  { h, r, k = __poly1305_setup_ref(k);
    r4444, r4444x5, r1234, r1234x5 = __poly1305_avx2_setup(r);
    in, inlen, h = __poly1305_avx2_update(in, inlen, r4444, r4444x5, r1234, r1234x5);
    in, inlen, h = __poly1305_update_ref(in, inlen, h, r);
    h2 = __poly1305_last_ref(in, inlen, k, h, r);
  }
  else
  { h2 = __poly1305_r_ref(in, inlen, k); }

  return h2;
}


inline fn __poly1305_avx2(reg u64 out in inlen k)
{
  reg u64[2] h2;
  h2 =  __poly1305_r_avx2(in, inlen, k);
  __store2(out, h2);
}


inline fn __poly1305_verify_avx2(reg u64 h in inlen k) -> reg u64
{
  reg u64[2] hc;
  reg u64 r;

  hc = __poly1305_r_avx2(in, inlen, k);
  r = __crypto_verify_p_u8x16_r_u64x2(h, hc);
  return r;
}

