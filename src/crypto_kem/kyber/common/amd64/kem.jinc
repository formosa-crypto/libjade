require "avx2/verify.jinc"

inline
fn __crypto_kem_keypair_jazz(reg u64 pkp, reg u64 skp) -> #msf reg u64
{
  stack u8[32] h_pk;
  stack u8[KYBER_SYMBYTES] rb;
  stack u64 s_skp s_pkp;
  reg u64 t64;
  inline int i;
  #msf reg u64 ms;

  s_pkp = pkp;
  s_skp = skp;

  ms = __indcpa_keypair(pkp, skp);

  skp = s_skp;
  skp = #protect(skp, ms);
  skp += KYBER_POLYVECBYTES;
  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  for i=0 to KYBER_INDCPA_PUBLICKEYBYTES/8
  {
    t64 = (u64)[pkp + 8*i];
    (u64)[skp] = t64;
    skp += 8;
  }

  s_skp = skp;
  pkp = s_pkp;
  pkp = #protect(pkp, ms);
  t64 = KYBER_INDCPA_PUBLICKEYBYTES;
  h_pk, ms = _isha3_256(h_pk, pkp, t64, ms);
  skp = s_skp;
  skp = #protect(skp, ms);

  for i=0 to 4
  {
    t64 = h_pk[u64 i];
    (u64)[skp] = t64;
    skp += 8;
  }

  rb = #randombytes(rb);
  ms = #init_msf();

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = rb[u64 i];
    (u64)[skp] = t64;
    skp += 8;
  }

  return ms;
}

inline
fn __crypto_kem_enc_jazz(reg u64 ctp, reg u64 shkp, reg u64 pkp /*, #msf reg u64 ms*/) -> #msf reg u64
{
  stack u8[KYBER_SYMBYTES * 2] buf kr;
  stack u8[KYBER_SYMBYTES] rb;
  stack u64 s_pkp s_ctp s_shkp;
  reg u64 t64;
  inline int i;
  #msf reg u64 ms;

  s_pkp = pkp;
  s_ctp = ctp;
  s_shkp = shkp;

  rb = #randombytes(rb);
  ms = #init_msf();

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = rb[u64 i];
    kr[u64 i] = t64;
  }

  buf[0:32], ms = _isha3_256_32(buf[0:32], kr[0:KYBER_SYMBYTES], ms);

  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  t64 = KYBER_PUBLICKEYBYTES;
  buf[KYBER_SYMBYTES:32], ms = _isha3_256(buf[KYBER_SYMBYTES:32], pkp, t64, ms);

  kr, ms = _isha3_512_64(kr, buf, ms);

  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  ms = __indcpa_enc_0(s_ctp, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES], ms);

  ctp = s_ctp;
  ctp = #protect(ctp, ms);
  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32], ms = _isha3_256(kr[KYBER_SYMBYTES:32], ctp, t64, ms);

  shkp = s_shkp;
  shkp = #protect(shkp, ms);
  t64 = KYBER_SSBYTES;
  ms = _shake256_64(shkp, t64, kr, ms);

  return ms;
}

inline
fn __crypto_kem_dec_jazz(reg u64 shkp, reg u64 ctp, reg u64 skp, #msf reg u64 ms) -> #msf reg u64
{
  stack u8[KYBER_INDCPA_BYTES] ctpc;
  stack u8[2*KYBER_SYMBYTES] kr buf;
  stack u64 s_skp s_ctp s_shkp;
  reg u64 pkp hp zp t64 cnd;
  inline int i;

  s_shkp = shkp;
  s_ctp = ctp;

  buf[0:KYBER_INDCPA_MSGBYTES], ms = __indcpa_dec(buf[0:KYBER_INDCPA_MSGBYTES], ctp, skp, ms);

  hp = skp + 32;
  hp += 24 * KYBER_K * KYBER_N>>3;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = (u64)[hp + 8*i];
    buf.[u64 KYBER_SYMBYTES + 8*i] = t64;
  }

  s_skp = skp;

  kr, ms = _isha3_512_64(kr, buf, ms);

  pkp = s_skp;
  pkp = #protect(pkp, ms);
  pkp += 12 * KYBER_K * KYBER_N>>3;

  ctpc, ms = __indcpa_enc_1(ctpc, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES], ms);

  ctp = s_ctp;
  ctp = #protect(ctp, ms);
  cnd = __verify(ctp, ctpc);

  zp = s_skp;
  zp = #protect(zp, ms);
  zp += 64;
  zp += 24 * KYBER_K * KYBER_N>>3;
  kr[0:KYBER_SYMBYTES] = __cmov(kr[0:KYBER_SYMBYTES], zp, cnd);

  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32], ms = _isha3_256(kr[KYBER_SYMBYTES:32], ctp, t64, ms);

  shkp = s_shkp;
  shkp = #protect(shkp, ms);
  t64 = KYBER_SSBYTES;
  ms = _shake256_64(shkp, t64, kr, ms);

  return ms;
}
