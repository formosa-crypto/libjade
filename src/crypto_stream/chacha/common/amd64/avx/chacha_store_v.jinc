
require "chacha_store_h.jinc" // __store_h_x2_avx __store_last_h_x2_avx


// t[0] = [17, 16,  1,  0]
// t[1] = [19, 18,  3,  2]
// t[2] = [21, 20,  5,  4]
// t[3] = [23, 22,  7,  6]
// t[4] = [49, 48, 33, 32]
// t[5] = [51, 50, 35, 34]
// t[6] = [53, 52, 37, 36]
// t[7] = [55, 54, 39, 38]
inline fn __sub_rotate_avx(reg u128[8] t) -> reg u128[8]
{
  reg u128[8] x;

  x[0] = #VPUNPCKL_2u64(t[0], t[1]);
  x[1] = #VPUNPCKL_2u64(t[2], t[3]);
  x[2] = #VPUNPCKH_2u64(t[0], t[1]);
  x[3] = #VPUNPCKH_2u64(t[2], t[3]);

  x[4] = #VPUNPCKL_2u64(t[4], t[5]);
  x[5] = #VPUNPCKL_2u64(t[6], t[7]);
  x[6] = #VPUNPCKH_2u64(t[4], t[5]);
  x[7] = #VPUNPCKH_2u64(t[6], t[7]);

  // x[0] = [3,   2,  1,  0] +0
  // x[1] = [7,   6,  5,  4] +16
  // x[2] = [19, 18, 17, 16] +64
  // x[3] = [23, 22, 21, 20] +80
  // x[4] = [35, 34, 33, 32] +128
  // x[5] = [39, 38, 37, 36] +144
  // x[6] = [51, 50, 49, 48] +192
  // x[7] = [55, 54, 53, 52] +208
  return x;
}



// x[0] = [48, 32, 16, 0]
// x[1] = [49, 33, 17, 1]
// x[2] = [50, 34, 18, 2]
// x[3] = [51, 35, 19, 3]
// x[4] = [52, 36, 20, 4]
// x[5] = [53, 37, 21, 5]
// x[6] = [54, 38, 22, 6]
// x[7] = [55, 39, 23, 7]
inline fn __rotate_avx(reg u128[8] x) -> reg u128[8]
{
  inline int i;
  reg u128[8] t;

  for i=0 to 4
  { t[  i] = #VPUNPCKL_4u32(x[2*i+0], x[2*i+1]);
    t[4+i] = #VPUNPCKH_4u32(x[2*i+0], x[2*i+1]); }

  x = __sub_rotate_avx(t);

  return x;
}


// same as __rotate_avx, but for stack
inline fn __rotate_stack_avx(stack u128[8] s) -> reg u128[8]
{
  inline int i;
  reg u128[8] t, x;

  for i=0 to 4
  { x[i] = s[2*i+0]; }

  for i=0 to 4
  { t[  i] = #VPUNPCKL_4u32(x[i], s[2*i+1]);
    t[4+i] = #VPUNPCKH_4u32(x[i], s[2*i+1]);
  }

  x = __sub_rotate_avx(t);

  return x;
}



inline fn __rotate_first_half_v_avx(reg u128[16] k) -> reg u128[8], stack u128[8]
{
  inline int i;
  stack u128[8] s_k8_15;
  reg   u128[8] k0_7;

  for i=0 to 8
  { s_k8_15[i] = k[8 + i]; }

  for i=0 to 8
  { k0_7[i] = k[i]; }

  k0_7 = __rotate_avx(k0_7);

  return k0_7, s_k8_15;
}



inline fn __rotate_second_half_v_avx(stack u128[8] s_k8_15) -> reg u128[8]
{
  reg u128[8] k8_15;
  k8_15 = __rotate_stack_avx(s_k8_15);
  return k8_15;
}



inline fn __interleave_avx(stack u128[8] s, reg u128[8] k, inline int o) -> reg u128[4], reg u128[4]
{
  reg u128[4] sk1 sk2;

  sk1[0] = s[o + 0];
  sk1[1] = s[o + 1];
  sk1[2] = k[o + 0];
  sk1[3] = k[o + 1];

  sk2[0] = s[o + 2];
  sk2[1] = s[o + 3];
  sk2[2] = k[o + 2];
  sk2[3] = k[o + 3];

  return sk1, sk2;
}


///////////////////////////////////////////////////////////////////////////////
// store 'xor' ////////////////////////////////////////////////////////////////


// 128 bytes (non sequentially)
inline fn __store_xor_half_interleave_v_avx(reg u64 output input, reg u128[8] k, inline int o)
{
  inline int i;

  for i=0 to 4
  { k[2*i  ] ^= (u128)[input + o + 64*i];
    k[2*i+1] ^= (u128)[input + o + 64*i + 16]; }

  for i=0 to 4
  { (u128)[output + o + 64*i     ] = k[2*i  ];
    (u128)[output + o + 64*i + 16] = k[2*i+1]; }
}


// 256 bytes
inline fn __store_xor_v_avx(reg u64 output input len, reg u128[16] k) -> reg u64, reg u64, reg u64
{
  stack u128[8] s_k8_15;
  reg u128[8] k0_7, k8_15;

  k0_7, s_k8_15 = __rotate_first_half_v_avx(k);
  __store_xor_half_interleave_v_avx(output, input, k0_7, 0);
  k8_15 = __rotate_second_half_v_avx(s_k8_15);
  __store_xor_half_interleave_v_avx(output, input, k8_15, 32);
  output, input, len = __update_ptr_xor_ref(output, input, len, 256);

  return output, input, len;
}


// <= 256 bytes
inline fn __store_xor_last_v_avx(reg u64 output input len, reg u128[16] k)
{
  stack u128[8] s_k0_7 s_k8_15;
  reg u128[8] k0_7 k8_15;
  reg u128[4] k0_3 k4_7;

  k0_7, s_k8_15 = __rotate_first_half_v_avx(k);
  s_k0_7 = #copy_128(k0_7);

  k8_15 = __rotate_second_half_v_avx(s_k8_15);
  k0_3, k4_7 = __interleave_avx(s_k0_7, k8_15, 0);

  if(len >= 128)
  { output, input, len = __store_xor_h_x2_avx(output, input, len, k0_3, k4_7);
    k0_3, k4_7 = __interleave_avx(s_k0_7, k8_15, 4);
  }

  __store_xor_last_h_x2_avx(output, input, len, k0_3, k4_7);
}


///////////////////////////////////////////////////////////////////////////////
// store //////////////////////////////////////////////////////////////////////


// 128 bytes (non sequentially)
inline fn __store_half_interleave_v_avx(reg u64 output, reg u128[8] k, inline int o)
{
  inline int i;

  for i=0 to 4
  { (u128)[output + o + 64*i     ] = k[2*i  ];
    (u128)[output + o + 64*i + 16] = k[2*i+1]; }
}


// 256 bytes
inline fn __store_v_avx(reg u64 output len, reg u128[16] k) -> reg u64, reg u64
{
  stack u128[8] s_k8_15;
  reg u128[8] k0_7, k8_15;

  k0_7, s_k8_15 = __rotate_first_half_v_avx(k);
  __store_half_interleave_v_avx(output, k0_7, 0);
  k8_15 = __rotate_second_half_v_avx(s_k8_15);
  __store_half_interleave_v_avx(output, k8_15, 32);
  output, len = __update_ptr_ref(output, len, 256);

  return output, len;
}


// <= 256 bytes
inline fn __store_last_v_avx(reg u64 output len, reg u128[16] k)
{
  stack u128[8] s_k0_7 s_k8_15;
  reg u128[8] k0_7 k8_15;
  reg u128[4] k0_3 k4_7;

  k0_7, s_k8_15 = __rotate_first_half_v_avx(k);
  s_k0_7 = #copy_128(k0_7);

  k8_15 = __rotate_second_half_v_avx(s_k8_15);
  k0_3, k4_7 = __interleave_avx(s_k0_7, k8_15, 0);

  if(len >= 128)
  { output, len = __store_h_x2_avx(output, len, k0_3, k4_7);
    k0_3, k4_7 = __interleave_avx(s_k0_7, k8_15, 4);
  }

  __store_last_h_x2_avx(output, len, k0_3, k4_7);
}


