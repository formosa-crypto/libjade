from Jade require "crypto_stream/chacha/common/amd64/ref/chacha_store.jinc" // update_ptr_ref update_ptr_xor_ref


///////////////////////////////////////////////////////////////////////////////
// store 'xor' ////////////////////////////////////////////////////////////////

// 'core' code for 2 blocks (128 bytes) ///////////////////////////////////////


// 128 bytes
inline fn __store_xor_h_avx2(reg u64 output plain len, reg u256[4] k) -> reg u64, reg u64, reg u64
{
  inline int i;

  for i=0 to 4
  { k[i] ^= (u256)[plain + 32*i];
    (u256)[output + 32*i] = k[i];
  }

  output, plain, len = __update_ptr_xor_ref(output, plain, len, 128);

  return output, plain, len;
}

// <= 128 bytes
inline fn __store_xor_last_h_avx2(reg u64 output plain len, reg u256[4] k)
{
  inline int i;
  reg u128 r0;
  reg u64 r1;
  reg u8 r2;

  // write 64 bytes
  if(len >= 64)
  { for i=0 to 2
    { k[i] ^= (u256)[plain + 32*i];
      (u256)[output + 32*i] = k[i];
    }
    output, plain, len = __update_ptr_xor_ref(output, plain, len, 64);
    k[0] = k[2];
    k[1] = k[3];
  }

  // write 32 bytes
  if(len >= 32)
  { k[0] ^= (u256)[plain + 0];
    (u256)[output + 0] = k[0];
    output, plain, len = __update_ptr_xor_ref(output, plain, len, 32);
    k[0] = k[1];
  }

  r0 = (128u) k[0]; // r0 = #VEXTRACTI128(k[0], 0);

  // write 16 bytes
  if(len >= 16)
  { r0 ^= (u128)[plain + 0];
    (u128)[output + 0] = r0;
    output, plain, len = __update_ptr_xor_ref(output, plain, len, 16);
    r0 = #VEXTRACTI128(k[0], 1);
  }

  r1 = #VPEXTR_64(r0, 0);

  // write 8 bytes
  if(len >= 8)
  { r1 ^= (u64)[plain + 0];
    (u64)[output + 0] = r1;
    output, plain, len = __update_ptr_xor_ref(output, plain, len, 8);
    r1 = #VPEXTR_64(r0, 1);
  }

  // write at most 8 bytes
  while(len > 0)
  {
    r2 = (8u) r1;
    r2 ^= (u8)[plain + 0];
    (u8)[output + 0] = r2;
    r1 >>= 8;
    output, plain, len = __update_ptr_xor_ref(output, plain, len, 1);
  }
}


///////////////////////////////////////////////////////////////////////////////
// 'core' code for 4 blocks (256 bytes) ///////////////////////////////////////

// 256 bytes
inline fn __store_xor_h_x2_avx2(reg u64 output plain len, reg u256[4] k1 k2) -> reg u64, reg u64, reg u64
{
  inline int i;

  for i=0 to 4
  { k1[i] ^= (u256)[plain + 32*i];
    (u256)[output + 32*i] = k1[i];
  }

  for i=0 to 4
  { k2[i] ^= (u256)[plain + 32*(i+4)];
    (u256)[output + 32*(i+4)] = k2[i];
  }

  output, plain, len = __update_ptr_xor_ref(output, plain, len, 256);

  return output, plain, len;
}


// <= 256 bytes
inline fn __store_xor_last_h_x2_avx2(reg u64 output plain len, reg u256[4] k1 k2)
{
  inline int i;

  // write 128 bytes
  if(len >= 128)
  { output, plain, len = __store_xor_h_avx2(output, plain, len, k1);
    k1 = #copy_256(k2);
  }

  __store_xor_last_h_avx2(output, plain, len, k1);
}


///////////////////////////////////////////////////////////////////////////////
// store //////////////////////////////////////////////////////////////////////

// 'core' code for 2 blocks (128 bytes) ///////////////////////////////////////


// 128 bytes
inline fn __store_h_avx2(reg u64 output len, reg u256[4] k) -> reg u64, reg u64
{
  inline int i;

  for i=0 to 4
  { (u256)[output + 32*i] = k[i]; }

  output, len = __update_ptr_ref(output, len, 128);

  return output, len;
}

// <= 128 bytes
inline fn __store_last_h_avx2(reg u64 output len, reg u256[4] k)
{
  inline int i;
  reg u128 r0;
  reg u64 r1;
  reg u8 r2;

  // write 64 bytes
  if(len >= 64)
  { for i=0 to 2
    { (u256)[output + 32*i] = k[i]; }
    output, len = __update_ptr_ref(output, len, 64);
    k[0] = k[2];
    k[1] = k[3];
  }

  // write 32 bytes
  if(len >= 32)
  { (u256)[output + 0] = k[0];
    output, len = __update_ptr_ref(output, len, 32);
    k[0] = k[1];
  }

  r0 = (128u) k[0]; // r0 = #VEXTRACTI128(k[0], 0);

  // write 16 bytes
  if(len >= 16)
  { (u128)[output + 0] = r0;
    output, len = __update_ptr_ref(output, len, 16);
    r0 = #VEXTRACTI128(k[0], 1);
  }

  r1 = #VPEXTR_64(r0, 0);

  // write 8 bytes
  if(len >= 8)
  { (u64)[output + 0] = r1;
    output, len = __update_ptr_ref(output, len, 8);
    r1 = #VPEXTR_64(r0, 1);
  }

  // write at most 8 bytes
  while(len > 0)
  {
    r2 = (8u) r1;
    (u8)[output + 0] = r2;
    r1 >>= 8;
    output, len = __update_ptr_ref(output, len, 1);
  }
}


///////////////////////////////////////////////////////////////////////////////
// 'core' code for 4 blocks (256 bytes) ///////////////////////////////////////

// 256 bytes
inline fn __store_h_x2_avx2(reg u64 output len, reg u256[4] k1 k2) -> reg u64, reg u64
{
  inline int i;

  for i=0 to 4
  { (u256)[output + 32*i] = k1[i]; }

  for i=0 to 4
  { (u256)[output + 32*(i+4)] = k2[i]; }

  output, len = __update_ptr_ref(output, len, 256);

  return output, len;
}


// <= 256 bytes
inline fn __store_last_h_x2_avx2(reg u64 output len, reg u256[4] k1 k2)
{
  inline int i;

  // write 128 bytes
  if(len >= 128)
  { output, len = __store_h_avx2(output, len, k1);
    k1 = #copy_256(k2);
  }

  __store_last_h_avx2(output, len, k1);
}


